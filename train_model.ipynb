{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES\n",
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "from pprint import pprint\n",
    "import scipy.io as sio\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras.models import Model\n",
    "import shutil as sh\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras import callbacks\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "#from matplotlib import pyplot as plt\n",
    "from keras import regularizers\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend as K\n",
    "\n",
    "# DEFINE FOLDERS\n",
    "train_data_dir = 'cars_train'\n",
    "val_data_dir = 'cars_test'\n",
    "nb_train_samples = 8144\n",
    "nb_val_samples = 8041\n",
    "\n",
    "#Creates an array with following values: picture name, picture category ID, train/validation label       \n",
    "def readData(matFile):   \n",
    "    content = sio.loadmat(matFile)\n",
    "    data = [(_[0][0][:],_[5][0][0],_[6][0][0]) for _ in content['annotations'][0]]\n",
    "    return data\n",
    "\n",
    "#Creates an array of all classes\n",
    "def readClasses(matFile):   \n",
    "    content = sio.loadmat(matFile)\n",
    "    classes = [(_[0]) for _ in content['class_names'][0]]\n",
    "    return classes\n",
    "\n",
    "#Movces raw data (pictures) into respective category subfolders with train/validation division \n",
    "def dataPreprocessing(dataDir, labelsFile):\n",
    "    data = readData(labelsFile)\n",
    "    classes = readClasses(labelsFile)\n",
    "    print(\"---------------\")\n",
    "    for recData in data:\n",
    "        if recData[2] == 1:\n",
    "            #validation set\n",
    "            os.makedirs(dataDir + \"/\" + val_data_dir + \"/\" + classes[recData[1] - 1] + \"/\", exist_ok=True)\n",
    "            sh.move(dataDir + \"/\" + recData[0][8:], dataDir + \"/\" + val_data_dir + \"/\" + classes[recData[1] - 1] + \"/\" + recData[0][8:])\n",
    "        else:\n",
    "            os.makedirs(dataDir + \"/\" + train_data_dir + \"/\" + classes[recData[1] - 1] + \"/\", exist_ok=True)\n",
    "            sh.move(dataDir + \"/\" + recData[0][8:], dataDir + \"/\" + train_data_dir + \"/\" + classes[recData[1] - 1] + \"/\" + recData[0][8:]) #train set\n",
    "\n",
    "#serializes the trained model and its weights\n",
    "def serializeModel(model, fileName):\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(fileName + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(fileName + \".h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "\n",
    "def prepareDataGenerators(batchSize, srcImagesDir, labelsFile):\n",
    "    classes = readClasses(labelsFile)\n",
    "# this is the augmentation configuration used for training\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1. / 255, \n",
    "        shear_range=0.2, \n",
    "        zoom_range=0.2, \n",
    "        horizontal_flip=True)\n",
    "# this is the augmentation configuration used for testing:\n",
    "# only rescaling\n",
    "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'car_ims_dir/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        srcImagesDir + \"/\" + train_data_dir + \"/\", # this is the target directory\n",
    "        target_size=(224, 224), # all images will be resized to 299x299\n",
    "        batch_size=batchSize, \n",
    "        class_mode='categorical') # since we use categorical_crossentropy loss, we need categorical labels\n",
    "# this is a similar generator, for validation data\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "        srcImagesDir + \"/\" + val_data_dir + \"/\", \n",
    "        target_size=(224, 224), \n",
    "        batch_size=batchSize, \n",
    "        class_mode='categorical')\n",
    "    return classes, train_generator, validation_generator\n",
    "\n",
    "\n",
    "def getVGG16Architecture(classes, dropoutRate):\n",
    "    # create the base pre-trained model\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    for layer in enumerate(base_model.layers):\n",
    "        layer[1].trainable = False\n",
    "    \n",
    "    #flatten the results from conv block\n",
    "    x = Flatten()(base_model.output)\n",
    "    \n",
    "    #add another fully connected layers with batch norm and dropout\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropoutRate)(x)\n",
    "    \n",
    "    #add another fully connected layers with batch norm and dropout\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropoutRate)(x)\n",
    "\n",
    "    #add logistic layer with all car classes\n",
    "    predictions = Dense(len(classes), activation='softmax', kernel_initializer='random_uniform', bias_initializer='random_uniform', bias_regularizer=regularizers.l2(0.01), name='predictions')(x)\n",
    "    \n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def getVGG19Architecture(classes, dropoutRate):\n",
    "    # create the base pre-trained model\n",
    "    base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    for layer in enumerate(base_model.layers):\n",
    "        layer[1].trainable = False\n",
    "    \n",
    "    #flatten the results from conv block\n",
    "    x = Flatten()(base_model.output)\n",
    "    \n",
    "    #add another fully connected layers with batch norm and dropout\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropoutRate)(x)\n",
    "    \n",
    "    #add another fully connected layers with batch norm and dropout\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropoutRate)(x)\n",
    "\n",
    "    #add logistic layer with all car classes\n",
    "    predictions = Dense(len(classes), activation='softmax', kernel_initializer='random_uniform', bias_initializer='random_uniform', bias_regularizer=regularizers.l2(0.01), name='predictions')(x)\n",
    "    \n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def getInceptionV3Architecture(classes, dropoutRate):\n",
    "    # create the base pre-trained model\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    \n",
    "    # InceptionV3\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # let's add a fully-connected layer\n",
    "    x = Dense(1024, activation='relu', kernel_initializer='random_uniform', bias_initializer='random_uniform', bias_regularizer=regularizers.l2(0.01))(x)\n",
    "    \n",
    "    # add Dropout regularizer\n",
    "    x = Dropout(dropoutRate)(x)\n",
    "    \n",
    "    # and a logistic layer with all car classes\n",
    "    predictions = Dense(len(classes), activation='softmax', kernel_initializer='random_uniform', bias_initializer='random_uniform', bias_regularizer=regularizers.l2(0.01), name='predictions')(x)\n",
    "    \n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    for layer in enumerate(base_model.layers):\n",
    "        layer[1].trainable = False\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def setLayersToRetrain(model, modelArchitecture):\n",
    "    \n",
    "    if modelArchitecture == 'InceptionV3':\n",
    "        # we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "        # the first 249 layers and unfreeze the rest:\n",
    "        for layer in model.layers[:249]:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        for layer in model.layers[249:]:\n",
    "            layer.trainable = True\n",
    "    elif modelArchitecture == 'VGG16':\n",
    "        #train the last conv block\n",
    "        for layer in model.layers[:15]:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        for layer in model.layers[15:]:\n",
    "            layer.trainable = True\n",
    "    elif modelArchitecture == 'VGG19':\n",
    "        #train the last conv block\n",
    "        for layer in model.layers[:17]:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        for layer in model.layers[17:]:\n",
    "            layer.trainable = True\n",
    "        \n",
    "\n",
    "\n",
    "def initialTraining(optimazerLastLayer, noOfEpochs, batchSize, savedModelName, train_generator, validation_generator, model, modelArchitecture, lr_decay, learningRate):\n",
    "    # compile the model and train the top layer only\n",
    "    \n",
    "    rms = RMSprop(decay=lr_decay, lr=learningRate)\n",
    "    model.compile(optimizer=rms, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    earlystop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, mode='auto')\n",
    "    history = model.fit_generator(\n",
    "        train_generator, \n",
    "        steps_per_epoch=nb_train_samples // batchSize, \n",
    "        epochs=noOfEpochs, \n",
    "        validation_data=validation_generator, \n",
    "        validation_steps=nb_val_samples // batchSize,\n",
    "        callbacks=[earlystop])\n",
    "    #plt.plot(history.history['val_acc'], 'r')\n",
    "    #plt.plot(history.history['acc'], 'b')\n",
    "    #plt.title('Performance of model ' + modelArchitecture)\n",
    "    #plt.ylabel('Accuracy')\n",
    "    #plt.xlabel('Epochs No')\n",
    "    #plt.savefig(savedModelName + '_initialModel_plot.png')\n",
    "    serializeModel(model, savedModelName + \"_initialModel\")\n",
    "\n",
    "\n",
    "def finetuningTraining(learningRate, noOfEpochs, batchSize, savedModelName, train_generator, validation_generator, model, lr_decay):\n",
    "    # we need to recompile the model for these modifications to take effect\n",
    "    # we use SGD with a low learning rate\n",
    "    sgd = SGD(lr=learningRate, decay=lr_decay, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    earlystop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, mode='auto')\n",
    "    \n",
    "    # we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "    # alongside the top Dense layers\n",
    "    history = model.fit_generator(\n",
    "        train_generator, \n",
    "        steps_per_epoch=nb_train_samples // batchSize, \n",
    "        epochs=noOfEpochs, \n",
    "        validation_data=validation_generator, \n",
    "        validation_steps=nb_val_samples // batchSize,\n",
    "        callbacks=[earlystop])\n",
    "    #plt.clf()\n",
    "    #plt.plot(history.history['val_acc'], 'r')\n",
    "    #plt.plot(history.history['acc'], 'b')\n",
    "    #plt.savefig(savedModelName + '_finalModel_plot.png')\n",
    "    serializeModel(model, savedModelName + \"_finalModel\")\n",
    "\n",
    "\n",
    "def model(learningRate, optimazerLastLayer, noOfEpochs, batchSize, savedModelName, srcImagesDir, labelsFile, modelArchitecture, dropoutRate, lr_decay):\n",
    "    \n",
    "    classes, train_generator, validation_generator = prepareDataGenerators(batchSize, srcImagesDir, labelsFile)\n",
    "\n",
    "    if modelArchitecture == 'VGG16':\n",
    "        model = getVGG16Architecture(classes, dropoutRate)\n",
    "    elif modelArchitecture == 'VGG19':\n",
    "        model = getVGG19Architecture(classes, dropoutRate)\n",
    "    else:\n",
    "        model = getInceptionV3Architecture(classes, dropoutRate)\n",
    "    \n",
    "    #Below is if I want to pre-load weights\n",
    "    #model.load_weights(\"carRecognition_finalModel.h5\")\n",
    "    \n",
    "    initialTraining(optimazerLastLayer, noOfEpochs, batchSize, savedModelName, train_generator, validation_generator, model, modelArchitecture, lr_decay, learningRate)\n",
    "    \n",
    "    setLayersToRetrain(model, modelArchitecture)\n",
    "    \n",
    "    finetuningTraining(learningRate, noOfEpochs, batchSize, savedModelName, train_generator, validation_generator, model, lr_decay)\n",
    "\n",
    "def main(args):\n",
    "    pprint(args)\n",
    "    if args.process_data:\n",
    "        dataPreprocessing(args.car_ims_dir, args.car_ims_labels)\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "    model(args.learning_rate, \n",
    "          args.optimizer_last_layer, \n",
    "          args.no_of_epochs, \n",
    "          args.batch_size, \n",
    "          args.saved_model_name,\n",
    "          args.car_ims_dir, \n",
    "          args.car_ims_labels,\n",
    "          args.model,\n",
    "          args.dropout_rate,\n",
    "          args.lr_decay)\n",
    "\n",
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "    \n",
    "def parse_arguments(argv):\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--process_data', \n",
    "        help='Divides the whole data set from Stanford (http://ai.stanford.edu/~jkrause/cars/car_dataset.html) into test and validation sets',\n",
    "        type=str2bool, \n",
    "        nargs='?',\n",
    "        const=True, \n",
    "        default=False)\n",
    "        \n",
    "    parser.add_argument('--car_ims_dir', type=str, \n",
    "        help='Directory where all pictures are located or where subfolder train/val are located', default='./car_ims')\n",
    "\n",
    "    parser.add_argument('--car_ims_labels', type=str, \n",
    "        help='Points to the file with all labels', default='./cars_annos.mat')    \n",
    "\n",
    "    parser.add_argument('--learning_rate', type=float,\n",
    "        help='Initial learning rate.', default=0.001)\n",
    "    \n",
    "    parser.add_argument('--dropout_rate', type=float,\n",
    "        help='Fraction of the input units to drop.', default=0.7)\n",
    "\n",
    "    parser.add_argument('--lr_decay', type=float,\n",
    "        help='Learning rate decay', default=1e-4)\n",
    "    \n",
    "    parser.add_argument('--optimizer_last_layer', type=str, choices=['ADAGRAD', 'ADADELTA', 'ADAM', 'RMSPROP', 'MOM'],\n",
    "        help='The optimization algorithm to use', default='RMSPROP')\n",
    "\n",
    "    parser.add_argument('--model', type=str, choices=['VGG19', 'VGG16', 'InceptionV3'],\n",
    "        help='The optimization algorithm to use', default='VGG16')\n",
    "\n",
    "    parser.add_argument('--no_of_epochs', type=int,\n",
    "        help='Number of epochs to run.', default=500)\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int,\n",
    "        help='Number of images to process in a batch.', default=64)\n",
    "    \n",
    "    parser.add_argument('--saved_model_name', type=str,\n",
    "        help='Number of images to process in a batch.', default='carRecognition')\n",
    "    \n",
    "    return parser.parse_args(argv)\n",
    "    \n",
    "#if __name__ == \"__main__\":\n",
    "    #main(parse_arguments(sys.argv[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(0.001, 'ADAM', 1, 64, 'carRecognition', '/Users/alexanderchung/Desktop/car2/', 'cars_annos.mat', 'VGG16', 0.8, 1e-4)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
